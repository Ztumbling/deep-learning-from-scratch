{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1F9cWv4I_6-hxRFMbHqQxhinMKSZAdPed",
      "authorship_tag": "ABX9TyN51MC8qUY2iLgCRINJaRUj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ztumbling/deep-learning-from-scratch/blob/master/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iyAmMWFbq87N",
        "outputId": "c1f2db70-6ca5-4752-b6c8-9bc091e98ae4"
      },
      "source": [
        "!pip install tensorboardX"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.4-py2.py3-none-any.whl (124 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20 kB 28.0 MB/s eta 0:00:01\r\u001b[K     |████████                        | 30 kB 18.3 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 40 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 51 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 71 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 81 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 92 kB 9.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 102 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 112 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 122 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 124 kB 7.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.19.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (1.15.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFBdcJP2Te7B",
        "outputId": "eb09b17e-061c-4b8e-9f2a-4d44b74a9759"
      },
      "source": [
        "cd ./drive/MyDrive/rmb_try/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/rmb_try\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V255c0mfSuId"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "from torch.utils.data import DataLoader\n",
        "import imp\n",
        "import Mydataset\n",
        "imp.reload(Mydataset)\n",
        "from Mydataset import RMBdataset\n",
        "import glob\n",
        "from torch import optim\n",
        "from tensorboardX import SummaryWriter"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmSqYyOCS_k0"
      },
      "source": [
        "EPOCH = 10\n",
        "BATCH_SIZE = 4\n",
        "LR = 0.01\n",
        "log_interval = 10\n",
        "val_interval = 1\n",
        "data_dir = './RMB_data'"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmXOTI3iTMaV"
      },
      "source": [
        "#split to 3 datasets\n",
        "path0 = os.path.join(data_dir,'1','*.jpg')\n",
        "path1 = os.path.join(data_dir,'100','*.jpg')\n",
        "jpg_list_0 = glob.glob(path0)\n",
        "jpg_list_1 = glob.glob(path1)\n",
        "final_info = []\n",
        "for i in jpg_list_0:\n",
        "    final_info.append((i,0))\n",
        "for j in jpg_list_1:\n",
        "    final_info.append((j,1))\n",
        "random.shuffle(final_info)\n",
        "len_data = len(final_info)\n",
        "\n",
        "train_info = final_info[0:int(0.8*len_data-1)]\n",
        "valid_info = final_info[int(0.8*len_data):int(0.9*len_data-1)]\n",
        "test_info = final_info[int(0.9*len_data):len_data]\n",
        "\n",
        "train_dataset = RMBdataset(train_info,1)\n",
        "valid_dataset = RMBdataset(valid_info,0)\n",
        "test_dataset = RMBdataset(test_info,0)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "valid_loader = DataLoader(dataset=valid_dataset, batch_size=BATCH_SIZE)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZ7CGAIpTOio"
      },
      "source": [
        "#create 模型\n",
        "class LeNet(nn.Module):\n",
        "    def __init__(self, classes):\n",
        "        super(LeNet, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3,6,5),\n",
        "            #nn.BatchNorm2d(num_features=6),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2,2),\n",
        "            nn.Conv2d(6,28,3),\n",
        "            #nn.BatchNorm2d(num_features=28),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2,2)\n",
        "        )\n",
        "        \n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(28*6*6,120),\n",
        "            #nn.BatchNorm1d(num_features=120),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(120,84),\n",
        "            #nn.BatchNorm1d(num_features=84),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(84,classes)\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size()[0], -1)\n",
        "        #x = x.resize(x.size()[0], 28, 6, 6)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight.data)\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight.data, 0, 0.1)\n",
        "                m.bias.data.zero_()\n",
        "                \n",
        "net = LeNet(2)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BzH0x7FXQX6"
      },
      "source": [
        "#Loss\n",
        "criterion = nn.CrossEntropyLoss()  \n",
        "#Optimizer\n",
        "optimizer = optim.SGD(net.parameters(),lr = LR,momentum=0.9)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1) "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQYZHaMBsYl3"
      },
      "source": [
        "iter_count = 0\n",
        "train_curve = list()\n",
        "valid_curve = list()\n",
        "for epoch in range(EPOCH):\n",
        "    net.train()\n",
        "    loss_mean = 0.\n",
        "    correct = 0.\n",
        "    total = 0.\n",
        "    for i, data in enumerate(train_loader):\n",
        "        iter_count += 1\n",
        "        #forward\n",
        "        inputs, labels = data\n",
        "        outputs = net(inputs)\n",
        "        #backward\n",
        "        optimizer.zero_grad()\n",
        "        loss = criterion(outputs,labels)\n",
        "        loss.backward()\n",
        "        #optimize\n",
        "        optimizer.step()\n",
        "        #add up\n",
        "        _,predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).squeeze().sum().numpy()\n",
        "\n",
        "        log_dir = \"./train_log\"\n",
        "        writer = SummaryWriter(log_dir=log_dir, filename_suffix=\"12345678\")\n",
        "        writer.add_scalars(\"Loss\", {\"Train\": loss.item()}, iter_count)\n",
        "        writer.add_scalars(\"Accuracy\", {\"Train\": correct / total}, iter_count)\n",
        "        #        # 打印训练信息\n",
        "        loss_mean += loss.item()\n",
        "        train_curve.append(loss.item())\n",
        "        if (i+1) % log_interval == 0:\n",
        "            loss_mean = loss_mean / log_interval\n",
        "            print(\"Training:Epoch[{:0>3}/{:0>3}] Iteration[{:0>3}/{:0>3}] Loss: {:.4f} Acc:{:.2%}\".format(\n",
        "                epoch+1, EPOCH, i+1, len(train_loader), loss_mean, correct / total))\n",
        "            loss_mean = 0.\n",
        "    for name, param in net.named_parameters():\n",
        "        writer.add_histogram(name + '_grad', param.grad, epoch)\n",
        "        writer.add_histogram(name + '_data', param, epoch)\n",
        "    scheduler.step()  # 每个 epoch 更新学习率\n",
        "    if (epoch+1) % val_interval == 0:\n",
        "        correct_val = 0.\n",
        "        total_val = 0.\n",
        "        loss_val = 0.\n",
        "        net.eval()\n",
        "        with torch.no_grad():\n",
        "            for j, data in enumerate(valid_loader):\n",
        "                inputs, labels = data\n",
        "                outputs = net(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total_val += labels.size(0)\n",
        "                correct_val += (predicted == labels).squeeze().sum().numpy()\n",
        "\n",
        "                loss_val += loss.item()\n",
        "\n",
        "            valid_curve.append(loss_val/valid_loader.__len__())\n",
        "            print(\"Valid:\\t Epoch[{:0>3}/{:0>3}] Iteration[{:0>3}/{:0>3}] Loss: {:.4f} Acc:{:.2%}\".format(\n",
        "                epoch+1, EPOCH, j+1, len(valid_loader), loss_val, correct_val / total_val))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpP7a5BRYTE5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2289eaa3-528b-4591-f54a-b1b82828eb41"
      },
      "source": [
        "##. TEST THE RESULT ##\n",
        "net.eval()\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=1)\n",
        "with torch.no_grad():\n",
        "    for k, data in enumerate(test_loader):\n",
        "        inputs, labels = data\n",
        "        outputs = net(inputs)\n",
        "        _, predicted = torch.max(outputs.data,1)\n",
        "        total_val += labels.size(0)\n",
        "        correct_val += (predicted == labels).squeeze().sum().numpy()\n",
        "print('Acc:{:.2%}'.format(correct_val/total_val))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Acc:100.00%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALsbPlle--c_"
      },
      "source": [
        "\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir=./train_log/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r86gHQZ6W6t5"
      },
      "source": [
        "!rm -fr ./train_log/"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jlwa-jstsf_S",
        "outputId": "e435d342-d8d3-4b80-c413-7a6455f8a7b1"
      },
      "source": [
        "    lenet = LeNet(classes=2)\n",
        "    import torchsummary\n",
        "    print(torchsummary.summary(lenet, (3, 32, 32), device=\"cpu\"))\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1            [-1, 6, 28, 28]             456\n",
            "              ReLU-2            [-1, 6, 28, 28]               0\n",
            "         MaxPool2d-3            [-1, 6, 14, 14]               0\n",
            "            Conv2d-4           [-1, 28, 12, 12]           1,540\n",
            "              ReLU-5           [-1, 28, 12, 12]               0\n",
            "         MaxPool2d-6             [-1, 28, 6, 6]               0\n",
            "            Linear-7                  [-1, 120]         121,080\n",
            "              ReLU-8                  [-1, 120]               0\n",
            "            Linear-9                   [-1, 84]          10,164\n",
            "             ReLU-10                   [-1, 84]               0\n",
            "           Linear-11                    [-1, 2]             170\n",
            "================================================================\n",
            "Total params: 133,410\n",
            "Trainable params: 133,410\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 0.15\n",
            "Params size (MB): 0.51\n",
            "Estimated Total Size (MB): 0.67\n",
            "----------------------------------------------------------------\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7Wybtyf4Htr",
        "outputId": "5ec6c0ae-5776-40b5-9ce4-fc7fb4702bf2"
      },
      "source": [
        "print(net)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LeNet(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Conv2d(6, 28, kernel_size=(3, 3), stride=(1, 1))\n",
            "    (4): ReLU()\n",
            "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=1008, out_features=120, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=120, out_features=84, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=84, out_features=2, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neX8ZPOgJ-R1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}